---
sidebar_label: "6.0 神经网络"
---

### 6.0 神经网络

Date：2022/10/22

------



[TOC]



------



* 芬兰赫尔辛基理工大学的教授给出的定义，使用最广泛
* 对我们的 ML 课程而言，==谈的网络只是神经网络里面很小的一个范畴==；例如，==数学家谈神经网络==，可能当作一个动力系统，研究它的收敛性、稳定性、不动点在哪等等；==自动控制的==，把神经网络作为一个控制器，研究神经控制层的行为等等；==我们==把神经网络当作一个学习算法，怎么通过数据来学习出模型，可以说我们研究的是神经网络学习，属于连接主义。【领域格局、工具定位，高屋建瓴】



------



### 6.1 神经网络模型

![image-20221022160457390](images/Task06/image-20221022160457390.png)

![image-20221022160806363](images/Task06/image-20221022160806363.png)

![image-20221022160859559](images/Task06/image-20221022160859559.png)

![image-20221022161026476](images/Task06/image-20221022161026476.png)

* Sigmoid 意思是 S 型，是一大类函数
* 讲对率回归的时候，用的 $\frac{1}{1+e^{-x}}$ ，如果求导，会得到一个有趣的形式 $f(x) = \frac{1}{1+e^{-x}}$ ，$f'(x) = f(x)(1 - f(x))$ 相当于正类概率与负类概率的积

![image-20221022161317248](images/Task06/image-20221022161317248.png)



![image-20221022161655172](images/Task06/image-20221022161655172.png)

![image-20221022161506624](images/Task06/image-20221022161506624.png)

* 最常见的结构；多少层？一种有歧义的说法是，根据功能层进行计数，输入层是没有 f 的，而上面两层有 f，可看作两层。另一种没有歧义的说法是：单隐（含）层。
* 前馈神经网络，可理解为同层内不存在环结构。

![image-20221022161903451](images/Task06/image-20221022161903451.png)

![image-20221022161914700](images/Task06/image-20221022161914700.png)

![image-20221022162108713](images/Task06/image-20221022162108713.png)



------



### 6.2 万有逼近能力

* 这一节讲得非常好
* Universal Approxinatary
  * 很多东西都有万有逼近性，例如傅里叶变换、泰勒展开；
  * 这里要特别指出，是因为曾经许多人怀疑机器学习的作用，这是机器学习有效性的前提条件——告诉你解是存在的，我们要做的是去找到它
  * 对于可计算的，图灵可解的问题，都可以用神经网络去逼近。

![image-20221022162448328](images/Task06/image-20221022162448328.png)

![image-20221022162744912](images/Task06/image-20221022162744912.png)

* 这题给出了定义上的重点内容：足够多、任意精度逼近、任意复杂度的连续函数

![image-20221022162905759](images/Task06/image-20221022162905759.png)

* 没有选 A 是因为考虑到以直代曲，将线性理解为 “直” 了；同时，对决策树的性质、意义、作用，理解不到位。

![image-20221022163016168](images/Task06/image-20221022163016168.png)



------



### 6.3 BP算法推导

![image-20221022163219819](images/Task06/image-20221022163219819.png)

![image-20221022163320228](images/Task06/image-20221022163320228.png)

![image-20221022163517170](images/Task06/image-20221022163517170.png)



![image-20221022163813127](images/Task06/image-20221022163813127.png)

![image-20221022163801159](images/Task06/image-20221022163801159.png)

![image-20221022163947779](images/Task06/image-20221022163947779.png)

![image-20221022164009814](images/Task06/image-20221022164009814.png)

![image-20221022164112778](images/Task06/image-20221022164112778.png)

* 这个过程是可以收敛的，与学习率有关
* 振荡现象（梯度下降，每次下降的位置；吴恩达讲过一个图）
* 轮 round

![image-20221022164414407](images/Task06/image-20221022164414407.png)

* 解释的角度跟吴恩达不同

![image-20221022164537969](images/Task06/image-20221022164537969.png)

![image-20221022164648032](images/Task06/image-20221022164648032.png)

![image-20221022164801433](images/Task06/image-20221022164801433.png)

* 错别字

* 【感悟】
  * 如何更好地理解过程信息？（回到 Scott 的书）



------



### 【06 EXAM】【计算题】

![image-20221022164835588](images/Task06/image-20221022164835588.png)

![image-20221022165102120](images/Task06/image-20221022165102120.png)

* 直接连接就是啥也没做，这是没有意义的

![image-20221022165227051](images/Task06/image-20221022165227051.png)

* 虽然不懂，但类比傅里叶变换的三角函数

![image-20221022165309638](images/Task06/image-20221022165309638.png)

![image-20221022165326134](images/Task06/image-20221022165326134.png)



![image-20221022165826050](images/Task06/image-20221022165826050.png)

![image-20221022170204735](images/Task06/image-20221022170204735.png)

![image-20221022170220010](images/Task06/image-20221022170220010.png)



![image-20221022170251430](images/Task06/image-20221022170251430.png)



* ==以下连续几题，需认真对待；考试前必须弄懂的内容==

![image-20221022171958852](images/Task06/image-20221022171958852.png)

* 我的计算只是套公式，很多不理解的、没用上的地方。
* 问题出在计算，搞不清楚对象、用什么公式、为什么用、有什么关系等基础内容。



![image-20221022170547110](images/Task06/image-20221022170547110.png)

* 要解这题，要知道的知识：
  * 目标值是什么（dE/dw 还是 dw？这里的 dw 是指导数还是微分中的 dw？）
  * 目标值的构建形式（链式法则）

![image-20221022170554371](images/Task06/image-20221022170554371.png)

![image-20221022170602267](images/Task06/image-20221022170602267.png)

![image-20221022170609911](images/Task06/image-20221022170609911.png)

* ==这里连续的几题，要好好做。慢下来理解之后才做。==

![image-20221024154121309](images/Task06/image-20221024154121309.png)

![image-20221030140713282](images/Task06/image-20221030140713282.png)

![image-20221030140720810](images/Task06/image-20221030140720810.png)

![image-20221030140728241](images/Task06/image-20221030140728241.png)

![image-20221030140737305](images/Task06/image-20221030140737305.png)

![image-20221030140745354](images/Task06/image-20221030140745354.png)

![image-20221030140752660](images/Task06/image-20221030140752660.png)



![image-20221022170641017](images/Task06/image-20221022170641017.png)

![image-20221022171257830](images/Task06/image-20221022171257830.png)



2022/10/22 18:51:40 1h23min + 30min = 1h53min

------

