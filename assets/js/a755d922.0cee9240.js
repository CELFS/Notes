"use strict";(self.webpackChunknotes_new=self.webpackChunknotes_new||[]).push([[9848],{3774:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var t=r(36672);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}},59306:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>h,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ML_Andrew_Ng/02_4_Week_Exercise","title":"02-4 Week Exercise","description":"Date\uff1a2022/04/27 346","source":"@site/docs/10_ML_Andrew_Ng/02_4_Week_Exercise.md","sourceDirName":"10_ML_Andrew_Ng","slug":"/ML_Andrew_Ng/02_4_Week_Exercise","permalink":"/Notes/docs/ML_Andrew_Ng/02_4_Week_Exercise","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"02-3 Week Octave-MATLAB","permalink":"/Notes/docs/ML_Andrew_Ng/02_3_Week_Octave_MATLAB"},"next":{"title":"03-1 Week Logistic Regression","permalink":"/Notes/docs/ML_Andrew_Ng/03_1_Week_Logistic_Regression"}}');var s=r(23420),a=r(3774);const i={},o="02-4 Week Exercise",h={},c=[{value:"\u3010QUESTIONS\u3011",id:"questions",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"02-4-week-exercise",children:"02-4 Week Exercise"})}),"\n",(0,s.jsx)(n.p,{children:"Date\uff1a2022/04/27 3:01:46"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"[TOC]"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"\uff082022/04/27 3:02:37 4h55min\uff09"}),"\n",(0,s.jsx)(n.h2,{id:"questions",children:"\u3010QUESTIONS\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:".*"})," VS ",(0,s.jsx)(n.code,{children:"*"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"==A*B => matrix multiply=="}),"\n",(0,s.jsx)(n.li,{children:"==A.*B => element-wise multiplication=="}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Math => Code"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-octave",children:">> A = [1 2; 3 4; 5 6]\r\nA =\r\n\r\n   1   2\r\n   3   4\r\n   5   6\r\n\r\n% *2 == .*2\r\n>> A .* 2\r\nans =\r\n\r\n    2    4\r\n    6    8\r\n   10   12\r\n\r\n>> A * 2\r\nans =\r\n\r\n    2    4\r\n    6    8\r\n   10   12\r\n\r\n\r\n>> x = [1; 2; 3]\r\nx =\r\n\r\n   1\r\n   2\r\n   3\r\n\r\n% .* == \u5bf9\u4f4d\u76f8\u4e58\r\n>> A .* x\r\nans =\r\n\r\n    1    2\r\n    6    8\r\n   15   18\r\n\r\n>> A * x\r\nerror: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x1)\r\n>> A(:,1) .* x\r\nans =\r\n\r\n    1\r\n    6\r\n   15\r\n\r\n>>\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cost Function"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-octave",children:"function J = computeCost(X, y, theta)\r\n%COMPUTECOST Compute cost for linear regression\r\n%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the\r\n%   parameter for linear regression to fit the data points in X and y\r\n\r\n% Initialize some useful values\r\nm = length(y); % number of training examples\r\n\r\n% You need to return the following variables correctly \r\nJ = 0;\r\n\r\n% ====================== YOUR CODE HERE ======================\r\n% Instructions: Compute the cost of a particular choice of theta\r\n%               You should set J to the cost.\r\n\r\npredictions = X * theta; \t% predictions of hypothesis on all m examples\r\nsqrErrors = (predictions - y) .^2;\t% squared errors\r\nJ = 1 / (2 * m) * sum(sqrErrors)\r\n\r\n% =========================================================================\r\n\r\nend\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gradient Descent"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-octave",children:"function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)\r\n%GRADIENTDESCENT Performs gradient descent to learn theta\r\n%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by \r\n%   taking num_iters gradient steps with learning rate alpha\r\n\r\n% Initialize some useful values\r\nm = length(y); % number of training examples\r\nJ_history = zeros(num_iters, 1);\r\n\r\nfor iter = 1:num_iters\r\n\r\n    % ====================== YOUR CODE HERE ======================\r\n    % Instructions: Perform a single gradient step on the parameter vector\r\n    %               theta. \r\n    %\r\n    % Hint: While debugging, it can be useful to print out the values\r\n    %       of the cost function (computeCost) and gradient here.\r\n    %\r\n    \r\n    \r\n    % d1 = sum(y - X * theta)* X(1)\r\n    % d2 = sum(y - X * theta)* X(2)\r\n    % theta(1) = theta(1) + alpha * d1\r\n    % theta(2) = theta(2) + alpha * d2\r\n    \r\n    %d1 = sum(X * theta - y)* X(1)\r\n    %d2 = sum(X * theta - y)* X(2)\r\n    %theta(1) = theta(1) + alpha * d1\r\n    %theta(2) = theta(2) + alpha * d2\r\n    \r\n    %h = X * theta - y\r\n    %theta(1) = theta(1) - alpha * sum(h * X(:, 1)') / m\r\n    %theta(2) = theta(2) - alpha * sum(h * X(:, 2)') / m\r\n    % theta = theta - alpha * sum((X * theta - y) * X) / m\r\n   \r\n    % theta(1) = theta(1) - alpha * sum(X * theta - y) * X(:,1) / m\r\n    % theta(2) = theta(2) - alpha * sum(X * theta - y) * X(:,2) / m\r\n    \r\n    temp = theta\r\n    for j = 1:2,\r\n      theta(j) = theta(j) - alpha / m * sum((X * temp - y) .* X(:, j)) \r\n    endfor\r\n    \r\n    % ============================================================\r\n\r\n    % Save the cost J in every iteration    \r\n    J_history(iter) = computeCost(X, y, theta);\r\n\r\nend\r\n\r\nend\n"})})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}}}]);