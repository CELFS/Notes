## 01 模块一：信息产生 04-18

Date：2021/12/31 13:53:52

------



[TOC]

------



### 04 信息的量化度量：世界上有稳赚不赔的生意吗

* 信息源，information sources
* 对于一条信息，重要的是找出其中有多少信息量，要搞清楚“信息量”，就要对信息进行量化的度量。
  * 香农找到了这个“砝码”——==“比特”==
    * 如果一个黑盒子中有 A 和 B ==两种==可能性，它们==出现的概率相同==，那么要搞清楚到底是 A 还是 B，==所需要的信息量就是一比特==。
    * 如果多于 A、B 两种可能性，更复杂的黑盒子，要消除它的不确定性需要多少信息？
      * eg：32个球队，知道答案的预言家——”是否答案在 A、B 中“——圈定范围；
  * 概念的本质
    * 充满不确定性的黑盒子——==信息源==
    * 里面的不确定性——==信息熵==
    * 消除不确定性的——==信息==
    * 要搞清楚黑盒子，需要——==信息量 = 信息熵==
* ①一个系统中的状态数量，也就是可能性，越多，不确定性越大；②在状态数量保持不变时，如果各个状态的可能性相同，不确定性就很大；③相反，如果个别状态容易发生，大部分状态都不可能发生，不确定性就小。
  * $H=-P_1logP_1-P_2logP_2-P_3logP_3-...-P_nlogP_n $ 
* ==**永远不要听那些正确率总是 50% 的专家的建议，因为那相当于什么都没说，没有提供能够减少“信息熵”的“信息量”。**==
* 你知道了信息有单位，还可以通过公式计算，那又有什么用呢？
  * eg：赌球，下注的人告诉设赌局的人；
  * 开赌局的，只要收费比信息实际的价值高，都是稳赚不赔的；
  * 这类赌局在金融市场更多，结构化的投资证券（Structured Notes）；
  * 金融数学专业；
* ==**多了解信息论和基本的数学常识，可以在生活中省下不少冤枉钱。**==
  * 要知道，很多交易和产品都是利用了信息的可度量性；
  * “信息过载”，判断一篇报道信息量多少；
  * 鸡汤文，没有信息量；
  * 相反的是大部分的诺奖论文。
* 总结
  * 香农，信息可以衡量，但不是用重要性，而是用信息量，单位是“比特”；
  * 可以把一个充满可能性的系统视为一个“信息源”，里面的不确定性叫“信息熵”，而“信息”就是用来消除这些不确定性的，所以搞清楚黑盒子里是怎么回事，需要的“信息量”就等于黑盒子里的“信息熵”。
  * 很多复杂交易背后其实都用到了信息的可度量性；
  * 信息量的大小不在于长短，而在于开创多少新知。
* 思考题
  * 如果你和一个特别会玩锤子、剪刀、布游戏的人玩这个游戏，你最好的策略是什么呢？



2022/01/09 18:30:31 23min

------



### 05 信息编码：数字和文字是如何诞生的

* 当数字多到划道道也无法表达时，就有了对数字的编码，也就是各种文明的数字——用有限数字的组合可以表示更多的数。
* 假设 100 个数，从中挑出一个，不确定性是 100 选 1，代表的信息熵为 $log100=6.65$ 比特。
  * 第一种编码，一一对应，即用 100 种符号；
    * 编码长度为一；
  * 第二种编码，采用十进制编码，也就是用 10 种符号。
    * 每个符号代表的信息量只有 $log10=3.325$ 比特，因此需要两两组合。
    * 两个符号的信息量加起来还是 6.65 比特。
* ==**对数字的各种编码其实是等价的，无非是平衡编码复杂性和编码长度之间的关系。**==
  * ==编码长度短，系统就复杂；==
  * eg：玛雅文明发展不快的一个原因，就和它的计数和书写系统太复杂有关。
  * eg：量子计算机；

> ==香农第一定律：==
>
> ​		编码长度   ≥   信息熵（信息量）/  每一个码的信息量

* 严格的数学证明，同时证明，只要编码设计得足够巧妙，上面的等号是成立的（07 最短编码）。
* 文字的诞生过程
  * 象形文字；
  * 出现用几个字表达一个复杂的含义；
  * 动词；
  * 书写系统；
  * 不平等加剧，能够认识编码的人，就掌握了其他人所没有的信息；
  * 能够读写的人，成了精英，甚至统治阶级；任何历史时期，==谁控制了信息，谁就是世界的主人。==
    * 马丁·路德之前，关于上帝的信息是由教士、主教控制的，农民只好受人摆布；
    * 中国情况也类似，不识字不过是土财主；
  * 简化的道路发展；
* 总结
  * 通过人类创造数字和文字语言的过程，告诉大家，其实它们都是人类用来==消除信息不确定性的编码手段==。各种编码系统，其实都是在编码==**复杂性**==和编码==**长度**==之间作平衡，它们在==数学上是等价的==；
  * 由于它们是==等价的==，所以，在一个编码系统中解决不了的问题，换一个系统统一解决不了；
  * 香农第一定律告诉我们，只要编码设计得足够巧妙，就可以找到==最短编码==。
  * 个体而言，==改变自己获取信息的能力，要比改变整个社会的不平等容易得多==。
* 思考题
  * 除了编码平衡，其实很多工作都是在作平衡，你的行业示范也有类似的平衡之道？
    * 规划行业，平衡政策、利益、设计之间的关系；



2022/01/09 18:54:47 24min

------



### 06 有效编码：10 个手指能表示多少个数字

* 信息编码的基本原理：
  * ==**“易识别”**==；
  * **==“有效性”==**；
* 多米尼克·穆特勒，清晰表达的五个原则：
  * 明确、诚实、勇气、责任、同理心。
    * 明确：沟通的核心，专业文件像八股文；
    * 诚实：明确表达的基础，否则口是心非；
    * 勇气：对一件事的态度确定，才能表达明确；
    * 责任：对一件事要有所谓，否则一天一个样。
* 十根手指，能表达多少个数字？
  * 伸开、收缩——1024；
  * 伸开、半伸开、收缩——59049；
  * ==**过分强调有效性，忽视了易辨识的原则，凡事过犹不及。**==
* 如何组合信息，保证它高效传递，还能不违背第一条“易辨识”的原则，就需要我们主动思考了。
  * eg：64瓶药，其中1瓶有毒，3天时间，最少需多少只小白鼠？（硅谷面试题）
    * 64 选 1的问题，6比特。【这种排列组合的有效编码，绝了】
  * ==**有效的编码，就是完成从理论上的上限到现实中解决方案的桥梁。**==
    * 很多 IT 问题，就是编码问题【因此好好学习《深入理解计算机系统》《信息量基础》涉及的编码问题】
    * ==让理论最佳值在应用中落地。==
    * 采用大量用户信息决定产品的设计和技术方案（用户大数据，二进制编码，同时进行几十个不同的不冲突的实验）
    * 一个公司将自己的商业成功寄托在“眼光好”上早晚要失败。
* 总结
  * 介绍了信息编码的两个基本原理：易辨识、有效性；
  * 用实例说明了信息论原理和工作的关系。
  * 听了一年课程没有提高？
    * 知识学习之后==只有真正使用了，才能变成自己的东西。==【要刻意使用】
    * ==学以致用比多学习更重要。==



2022/01/09 19:16:59 22min

------



### 07 最短编码：如何利用哈夫曼编码原理投资

* 如何对信息进行编码才最有效？
  * eg，摩尔斯电码：根据常识对经常出现的字母采用较短的编码，对不常见的字母用较长的编码，这样可以降低编码的整体长度。

<img src="D:\TyporaTxt\PicCopy\image-20211231135601344.png" alt="image-20211231135601344" style="zoom: 67%;" />

* log26 大约 5 bit信息。莫尔斯的编码方法，平均只需 3 bit，省约 1/3 时间。
  * eg：谍战片报务员，敌方特工，二战欧洲德占区，省一点时间意味着自身的安全；【沟通表达也如此，少说额外的话，同样可以降低不必要的风险】
  * eg：各国长途电话区位码，重要的城市用更短的位数。
* **可以证明，越常出现的信息采用较短的编码，不常出现的信息采用较长的编码，就能比采用同样码长的信息总体上更合算。**
  * 【这种思维可以运用在许多地方，比如笔记的编号长度，就可以缩短、分类，以后复盘经常看的内容，就置顶；】
  * 【投资也一样】
  * 这种最短编码方法——等于香农第一定律的继续，最早由 MIT 哈夫曼教授发明；
* ==哈夫曼编码，三个要点：==
  * 1、香农第一定律——==编码长度有个理论最小值==，数学可以证明哈夫曼的编码方法是最优化的。
  * 2、本质上，是将最宝贵的资源==（最短的编码）给出现概率最大的信息==。至于资源如何分配，哈夫曼给出了一个原则，也就是一条信息编码的长度和出现概率的对数成正比。
    * 概率 1/2，$log_2{1/2} = -1$ 因此，编码长度为 1，即码 0，最后两条出现概率 1/2^31次方，对数后等于 -31，因此编码长度 31。【这样具体说明才理解了哈夫曼编码的一些核心思想，之前 KKB 课程讲得就挺抽象难懂的】
    * 莫尔斯码是无意中用了哈夫曼编码的原理，但没有严格统计，因此非最优化。
  * 3、==现实生活中，很多信息的组合==，比如单独一条信息，==其概率分布差异更大，==因此对它们使用哈夫曼编码进行信息压缩，==压缩比==会更高。
    * eg：汉语，汉字频率统计，压缩，一篇文章通常可压缩 50% 以上，若按词频统计，再用哈夫曼编码，可压缩 70% 以上。
* 生活中的哈夫曼编码原则：
  * 凯鹏华盈；它不是靠一两个人天才的眼光，而是有一整套系统的方法，保证投资的成功率。
    * 通过每一次双倍砸钱（double down），把最多的钱投入到最容易成功的项目上。
    * 示例
      * 平均地投入到 100 个初创公司；（市场平均回报，2 ~ 5年 / 7 ~ 10年，最终 20% ~ 50%，年化 5% ~ 20%）
      * 利用我们的眼光投入到一家最可能的公司中；（赌博，145-148封信）
      * 利用哈夫曼编码原理投资。

<img src="D:\TyporaTxt\PicCopy\image-20211231151238900.png" alt="image-20211231151238900" style="zoom:67%;" />

* 最后能变成产品上市的，是少数项目，但是大量的资源投入在其中了。
* 将教育比作用圆规画圆，一方面有一个扎得很深的中心，另一方面又足够广的很浅的覆盖面。
* ==吴军老师的经验：==
  * 我**==从来不排斥尝试新东西，这样不会失去机会==**，我尝试过的各种事情远比外界知道的多，只是绝大部分失败了，我没有继续罢了，大家也就无从知晓了。
  * 对于花了一些精力，看样子做不成的事情，我是**==坚决做减法止损==**的。【服务器编程、算法深入等，觉得学不好的，可以先放一放，核心的还是要学，只不过是在需要的时候结合起来，更高效地学】
  * 可以把**==最多的资源投入到我擅长的，有兴趣的，可能也是成功率最高的==**事情上。



2021/12/31 15:20:42 1h31min

------



### 08 矢量化：象形文字和拼音文字是如何演化的

* 语言和文字是**慢慢演化过来的**，而不是人为利用信息论的编码原理刻意构造的，因此不可能只照顾易辨识和有效性，**而不考虑人类接受它们的难度**，以及演化的过程。

* 相反，人类给计算机识别的单词，比如汇编语言的指令代号，基本上就很短的、等长的字母组合，因为**那是完全利用编码原理人工设计的**。
* 信息的矢量数字化（矢量化，VQ | 简化的自然过程）
  * 信息越多，需要的编码越多，这是**文明自然演变不可避免的过程**。
  * 太多不同的编码（文字）出现后，就要对编码进行简化，否则没法学习。【顶层】
  * bitmap、VQ
  * eg：图形彼此有些相似性，但有不完全一样——**==特征分类，投射到两个或更多的维度，维度的组合可以决定面上的一个矢量；==**
  * eg：人类象形文字的演化过程同理；
  * eg：汉字为例，绝大多数映射到两个维度
    * 一个表意的偏旁维度；
    * 一个提示读音的发音维度（有时也表意）；
    * eg：《康熙字典》47000个汉字，但3000多个一级国标汉字已能覆盖98%以上的文本；
    * eg：《牛津词典》约1/4词今天已不用了；
    * 更习惯于用现有的字发明新词，而不是造字；
* ==**拼音文字的矢量化过程**==【这样表述很好理解，因为逻辑完整、闭合，有关键事件抓手】
  * 美索不达米亚人——楔形文字（象形文字 -> 拼音文字 “箭头”）
  * 闪米特人 . 腓尼基人——地中海——商人——简化——剩几十个字母
  * 腓尼基字母——希腊人——总结24个希腊字母——罗马人——22拉丁字母
  * 罗马扩张——外国人 . 人名、地名无法表示——加入x（无法表示的音、词 | 包含x的单词特别少的原因，未知数x同理）
  * 拉丁文
    * i——i、j
    * v——u、v、w
  * 最终形成 26 字母；
  * 词根、前缀、后缀——三维矢量表示；【**大部分词都可以根据这样的映射来学**】

* 虽然象形文字和拼音文字的形成和进化代表了两种不同的信息编码方式，**==但都利用了信息论中矢量化的原理。==**
  * 矢量字体；
    * 将字体轮廓映射到一组曲线上，显示/打印时，数学运算，恢复形状；
  * 高考成绩录取，身高选拔球员——**映射到一维的空间**；
    * **==这种做法给工作带来极大的便利性，但显然没有全面地考察每一个人，或者说有信息的缺失。==**
    * 更普遍意义的问题——矢量化会带来多大的信息缺失（信息论可计算）；
  * 工程
    * **如何平衡便利性和信息上的缺失；**
    * **理论上办不到的；**

* 总结
  * 我们从文字的演变，介绍了信息的矢量化的概念，以及它的应用；
  * 进而讲述了，无论是象形文字还是天然形成的拼音文字，都通过两到三个维度的矢量化兼顾了读音和达意的关系。但是，如果强制将中文拼音化，它将失去达意的功能，这不符合信息论的原则，因此做不下去。==**世界上人为想做的，但违背规律的事情，做起来总是困难重重。**==
  * 生活很多矢量化例子，**==它们让问题变得简单，但会丢失信息，而平衡便利性和信息的完整性，就成为了艺术。==**



2022/01/10 23:19:26 34min

------



### 09 冗余度：《史记》和《圣经》哪个信息量大

* 量化度量《史记》和《圣经》的信息量——**==信息压缩实验==**
  * 选择原因：这样经典的典籍==语言相当精炼==，比较有==代表性==，而且在==翻译成不同语言的版本==时，都==极为仔细准确==。相比，文学作品的翻译随意性就较大。
  * 《圣经》
    * 80万单词，扣除空格、标点，存储约 4MB；
    * ASCII 编码，长度四百万字节；
    * “和合本” 中译本，93 万字；国标汉字存储，约 2MB；
    * 挤掉 ASCII 编码的水分，中英编码长度差约 30%；
    * 对比 2 : 3，1.6MB VS 2.5MB；
    * 哈夫曼编码，中英均可压缩到约 750KB；
    * 进一步利用上下文，可进一步压缩（不考虑）；
    * ——证实了一本经典的信息量不会因为使用不同语言书写而不同，也证实了编码的等价性——**==同样的信息不同的编码，信息量不变==**。
      * 英语压缩比 3 : 1
      * 中文压缩比 2 : 1
  * 《史记》
    * 目的：对照实验，考虑到 “和合本” 的句子和日常说话方式差异较大；
    * 53 万字，国标码存储 1.1MB；挤掉水分 编码长度约 900KB；
    * 哈夫曼编码，不到 500KB；
    * 压缩比 1.8 : 1

![image-20220111174539537](D:\TyporaTxt\PicCopy\image-20220111174539537.png)



> **冗余度：**对信息的 “密集” 和 “稀疏” 程度进行描述。
>
> ​		（信息的编码长度 - 一条信息的信息量）/ 信息的编码长度

* ==**冗余度太低，会严重影响接收信息的速度。**==
  * eg：小说冗余度高得多，才容易阅读；**==同语言，不同题材，冗余度差很多；==**
  * eg：沃森、克里克论文，每个单词都不能漏掉，理解反而花时间；
  * 【是否意味着，在理解比较专业的信息时，可以**==自行增加或者科学地增加冗余度，从而帮助提升学习效率呢？==**在教授给别人知识的时候，讲故事是一个好办法。】
* **冗余度的好处**
  * **==便于理解；==**
  * 语言学上，**==消除了很多歧义性==**；
    * 汉语：去掉动词的时态、性别、单复数、语气等信息，名词去掉了数量、阴阳信息，绝大部分名词去掉了正式和非正式的信息——这些都需要通过上下文来恢复——恢复得不好，理解略有差别，造成误解。
    * 拉丁语、法语（极为严谨）：英语，名词、动词数量一致性，语句的语气、写法一致性，保证了相应信息不容易漏掉——即归功于冗余度大。
  * ==**带来信息的容错性；**==
    * 文件丢失一段，仍然能得到大部分内容，甚至通过前后恢复出一部分丢失的内容；
    * eg：罗塞塔石碑，拿破仑，商博良；尼罗河流域五千年文明的面纱揭开；

* 冗余度的弊端
  * **==存储和传递信息时的浪费；==**
    * 4K 冗余度极高，压缩几十倍也不会损失人任何信息；
  * 如果信息中**==混有噪音==**，过多没用的信息可能会**==导致错误==**；
    * 真实世界，很少有绝对干净的信息，总是混有噪音，可能彼此矛盾，产生糊涂；
    * eg：《史记》秦王子婴，可能司马迁也拿不准，因此把常见的说法都列出来，让读者自己判断；——自相矛盾冗余信息多了，说话不靠谱；

* 善用信息冗余，沟通高手
  * 1、讲东西加入一些**==看似废话==**，实际上是**==从侧面诠释==**你的想法的例子，帮助对方理解你的意思。比如**==“换句话说”，“比如说”，“从另一方面讲”==**，就是利用信息冗余便于大家理解。
  * 2、**==讲东西要有一致性==**，不要补充有可能和主要思想相矛盾的例子，或者和想法无关的冗余信息。
  * 3、脑子存储信息时，要进行压缩，这样脑子才记得住事情。很多人问题，你读那么多书，记那么多事情，怎么记得住？
    * **==无论读书、学习，都会做类似写卡片的工作，把厚厚一本书，变成薄薄的几页纸，那些冗余的信息，就删除掉了。==**
    * **==读书要不求甚解——读出主线，将一些细节过滤掉。==**真需要寻找细节时，大不了回头再看看。
* 总结
  * 冗余度的概念，并通过冗余度证明了汉语是最简洁的语言，但同时也说明了因为汉语**==冗余度太低理解起来比较困难==**，因此难以学习。【此处想起来《鸟哥的 Linux 私房菜》以“啰嗦”但有效著称】
  * 介绍了冗余度三个好处：**==易理解、消歧义、容错性。==**
  * 冗余度的问题：造成信息**==存储和传输的浪费==**；有**==噪音==**情况下，可导致**==混淆==**。
  * 读书、学习
    * ——想办法**==把书读薄==**，即去掉冗余的信息；【最好的办法就是随笔】
    * 不求甚解——**==读出主线、过滤细节==**，大不了需要时**==多看几遍==**。
* 思考题
  * 怎样写文章才能让大家接受信息的效率最高呢？



2022/01/11 18:38:39 1h22min 聊书 + 36min

------



### 10 等价性：信息是如何压缩的

* eg：《史记》秦王子婴到底是谁
  * 演绎了，面对错综复杂的信息时，**==如何利用其他信息的等价性为我们理清思路==**。
* 倒手一次的操作需要一个==桥梁==，让**原有和等价信息一一对应**。
  * eg：傅里叶变换，十九世纪，法国；——**==任何周期性的函数（信号）都等于一些三角函数的线性组合。==**

> **傅里叶变换：**所有的周期性信号都可以用**频率和振幅不同的正弦函数叠加**而成。

* 也就是说周期性信号里面所包含的信息和若干正弦函数的频率、振幅信息**==完全等价==**。

* 可利用周期性来进行信息压缩，基本原理：
  * 找到周期性信号的**==等价信息==**；（关键）
  * 对等价信息进行压缩；
  * 如要使用原来的信号，通过压缩后的等价信息复原原来的信号。
* 正弦波，振幅、频率；
* eg：100年温度变化信息，20根频率、振幅不同的正弦曲线叠加而成，压缩百倍。
* eg：音频信号；宏观看到一幅图，放大可以看到，相邻像素之间颜色和灰度的变化回事相对连续的。
  * 利用这个原理，发明了工具——**==离散余弦变换，DCT==**
    * 64 个基本灰度模板；JPEG 算法、格式图片；
    * RGB 三原色的彩色模板；
    * 信息损失；

* 一种原始信息，高度冗余成分，很难直接压缩——将其转化为容易压缩的等价的信息，进而压缩、存储、传输。——接收端，解压、恢复原来信息。
  * Google 搜索；
  * 核磁共振原理；
  * LIGO 装置；
  * ATLAS 装置；
  * 血项检查。

* 总结
  * 信息压缩，说明**==等价信息的重要性==**，对于获取信息、处理信息同样重要。

* 思考题
  * 你是如何通过等价性原则找出一些难题的答案的？【这是一个相当有趣的问题，关键在于对过往知识的复盘意义】



2022/01/12 19:08:50 23min

------



### 11 信息增量：信息压缩中的保守主义原则

* 3210、3208、3206、3211、3220、3212……进行编码，需多少比特（或字节）？
  * 毫无规律，不存在频率大小问题，无法哈夫曼编码；只好一一编码。【特征】
  * 这组十字都是三千多，需 12 位 2 进制表示每一个数字；【信息量】
    * 即每个数字编码长度 12 比特；【编码长度】——0~4095【4096 - 1】
    * 若 11 位二进制，2^11 = 2048，不能涵盖 3000 多的数字；【确定边界】
  * 各数字变化不大，动态范围不超过 16——可按此特性压缩编码【变化特征，增量】
    * 1、对第一个数字使用 12 比特的编码，没有办法更精简；
    * 2、对第二个以后的各个数字，将它和上一个数字相比较，发现比前一个数字，动态变化范围在正负 16 以内。因此只需要对差异（也叫增量）进行增量编码。
      * 对于增量，若不考虑符合，用 4 比特就够（log16 = 4）
      * 正负，加 1 比特信息表示符号；
      * 因此，第二个字开始，**==采用 5 个比特就可以表示和前一个数字的区别了。==**

* 3120【-2】【-2】【5】【9】【-8】……
  * 【突然想起高中数学课上，加权平均的简便算法，就是在平均数的基础上，只计算每一个值相对的增减，即变化量；没想到这种变化的艺术居然可有大用】
  * 压缩比 12 : 5——2.4 : 1
  * **==解码时，==**先解出第一个，再解出后面的增量，再根据上一个的数值和当前的增量，恢复出一个个原来的信息。
    * eg：今天对视频的压缩原理；
      * 第一帧（主帧）——全画面编码；
      * 后面每一帧，针对和上一帧的差异进行编码即可。
    * eg：Google 搜索的索引——前后相关性压缩；
      * 每一个单词在全部网页中出现的位置列出来；
        * 否则，几百亿排序位置；
      * 每一个网页只保存第一个单词的起始位置，剩下单词相对第一个单词的位置；
        * 同时包含关键词，看是否有共同的网页起始位置即可；
        * 连起来的词，保证同起始，还保证位移量相差 1；
      * 节省空间（约省 75%），无损压缩；
* ==**弊端——挤掉信息冗余，编码长度非常短，容错性能下降；**==
  * eg：光盘，划一道，跳盘，后面都看不了；
    * ==策略：**每过若干帧，重新产生一个主帧，以免错误传递太远。**==【山火隔离带同理】
* 信息的前后相关性，其实是信息本身固有的特征。或者说，绝大多数时候，我们这个世界的变化是渐进的，而不是完全随机的。不仅在信息的世界如此，在我们的生活中也是如此。
  * eg：**==保守主义的做事态度，好处是由我们这个世界渐变的特征决定的。==**绝大多数，不需推倒重来，只需对变化进行一些修补就好了。
    * 有些人看不起总是修修补补的做法，缺乏革命性；
    * 信息论角度，保守主义做法成本最低。
  * ==**平衡各方面利益不断修补的结果，若过时如同重新设一段主帧的思想**==
    * eg：美国税法；
    * eg：学区划分犬牙交错；
    * eg：美国宪法修正案；
  * ==**正是因为渐进，牵扯的利益不会太多，才能够推行得下去，从长期来看才能发展。**==
    * 【学习也是渐进的过程，有了记录的基础，就不用每次推倒重来】

* **==如果想一次完成巨大的突变，常常会因为牵扯的利益太多，最后总是搁浅，永远改不了，结果反而是不进步。==**

* 总结
  * 讲了善用信息前后的相关性，对于后面的信息做增量编码，达到大幅度压缩信息冗余的目的。
  * 把这种信息处理的方式，和保守主义的做事方法对比。保守主义，坚持总体原则不变，不断作微调，达到渐进改变的目的。效率更高，因为世界客观性。
* 思考题
  * 如果我们在视频压缩中有一个主帧的概念，后面各帧图像都是对它作对比，那么在我们的生活，什么事情其实扮演了主帧的角色？



2022/01/14 13:56:40 1h12min

------



### 12 压缩比和失真率：如何在信息取舍之间作平衡

* 任何编码的长度都不会小于信息熵，也就是通常会大于等于信息熵，当然最理想的是等于。
  * 如果编码长度太短，小于信息熵，就会出现损失信息的现象。
    * 无损压缩
      * eg：傅里叶变换、离散余弦变换，转变为频率，再类似哈夫曼编码压缩；
    * 有损压缩
      * 今天绝大多数是有损压缩；
      * 最关键：清楚如何保证因为压缩而丢失的信息不影响我们对信息的理解。
        * 需平衡压缩比和信息失真度之间的关系。
* 三个原则
  * 世界上很多时候没有最好的技术方案，只能==根据场景找到合适==的，因此==**做事的目的性很重要**==。
  * 丢失一部分信息，**==一定会增加不确定性==**。用的信息少，永远不可能做得和原来一样好。
    * 平衡的做法：看矛盾的主要方面在哪一方。
    * 今天世界最不缺的是数据；
  * 在压缩信息时，有时要**==看应用场景==**。
    * eg：语音压缩 VS 声纹识别；
* 高比例的信息压缩到底压缩掉什么信息？
  * 压缩掉**==高频信息==**；
    * eg：蓝天、小鸟；JPEG算法压缩，小鸟可能被过滤掉，蓝天细节就没有了；
    * eg：枪打出头鸟；
    * **==任何与众不同的东西，总是先被压缩掉==**，因为对这些东西编码，占用空间相对太多。
    * 【高频信息，指的就是与众不同的信息，可以想象一张图片的画面，蓝天中突然出现一只小鸟，其频率由表示连续蓝天的平缓，突然产生较大幅度的变化，因此就变成频率不同的信息，至于高低，应该有一个基准线，后面深入了解再细化】
  * eg：华大基因，杨焕明教授，每个人基因测序，超 1PB 存储空间，1000 个 1T硬盘；
  * eg：约翰·霍普金斯大学，遗传压缩算法；
    * 只要存储有差异的基因即可；

* 总结
  * 信息的压缩分为有损和无损的两种。
    * ==无损压缩：==原信息可完全复原，压缩比通常不高，因此存在极限（香农第一定律，信息熵极限）；
    * ==有损压缩：==信息复原，一定程度的失真。
  * 失真率和压缩比直接相关，压缩比越大，失真率越高。
    * 何种压缩==方法==，压缩到何种==程度==，看具体应用==场景==；
    * 信息处理领域，常不存在所谓标准答案、最佳答案，**==只有针对某个场景的好的答案，而一切都是妥协的结果==**。
  * 信息压缩的**==思想可以用到很多地方==**，把知识学通，就是这个道理。

* 思考题
  * 无损音乐、无损电影爱好者，从听觉和观感上说说，真的有那么大区别吗？有没有其他影响因素呢？



2022/01/15 18:57:48 27min

------



### 13 信息正交性：在信息很多的情况下如何作决策

* 信息特点
  * 不守恒
  * 不同信息彼此的关系，使用相同或不同信息带来的结果
  * 这些方面都和能量完全不同
  * 相同的信息使用两次就没用了，也不会有害；

* 如何发挥信息叠加的力量呢？
  * 信息是正交（垂直）的时候，效果最好
  * eg：NIST 测试语音识别报告，数据表明，看似来源不同的信息，在消除不确定性时，作用有可能重叠。

* 什么是垂直的信息？（任何信息可对应某空间中的一个矢量，夹角 90 度）
  * eg：语音识别，本质 N 选 一问题。
    * 第一类，语音信息，==每一个==读音和各种语音==之间的相关信息==。
    * 第二类，语言信息，==一种==读音在上下文中==出现的可能性==。
  * eg：名片识别
    * 市面大部分 98% 识别率，很高；
    * 2012，加州大学华裔教授，大数据方法；把互联网能找到的各个单位的信息找到，用公开信息验证图像识别的结果。

* 如何找到正交的信息？【三个原则】
  * **==不同的信息要来自不同的信息源。==**
    * eg：血项检查、医学影像扫描；
      * X 光透视、CT 扫描、核磁共振；基本是同一维度的；
    * 容易上当的一个原因：**==不善于选用正交的信息进行交叉验证。==**
    * eg：媒体信息相互抄，一种信息多次使用；
  * ==**避免反复使用相互嵌套或相互包含的信息。**==
    * 即便信息源不同，覆盖，相似性太高；
    * eg：简历，容易提供相互覆盖的信息，无关紧要的工作经历，可有可无的专业证书；
  * ==**看问题要可以改变一下观察的角度，从几个不同的角度看。**==
    * eg：圆柱体的侧面与俯视；

* 如何选取几种最重要而且彼此尽可能正交的信息呢？【两个方法】
  * **==不断叠加。==**
    * 第一步，单独评估，从大到小排列，第一作为基准；
    * 第二步，在第一种已用基础上，重新评估剩下的，排序，选出最高；
    * 第三步，循环。
    * 衡量的不是每一种信息单独的有效性，而是找到它们组合的有效性。
    * eg：篮球球员选拔。
  * **==不断删除。==**
    * 类似上述，逆向过程。
  * 局限性：
    * 都有可能陷入一种局部最佳值，至今也没得到彻底解决，成功有运气成分。

* 总结
  * 力学上，用力要在一个方向效果才好，在使用信息上，要选用彼此垂直的正交信息。这其实是换一个角度看问题背后的科学原理。
  * 给出选择正交信息的 3 个原则，2 个做法。



2022/01/16 16:33:54 39min

------



### 14 互信息：相关不是因果

* 衡量两条信息之间相关性的新工具：互信息——量化度量相关性的方法。
* **==相关不是因果==**，其实世界上大多数联系都是相关联系，而非因果联系。相关联系可以强，可以弱，但**弱相关其实没有什么意义**，我们**==需要寻找和利用的是强相关性==**。
  * eg：裙摆指数；
    * 马尔基尔《漫步华尔街》，专门论述，认为这个牵强的相关性毫无根据。

* 互信息公式

<img src="D:\TyporaTxt\PicCopy\image-20220117163051863.png" alt="image-20220117163051863" style="zoom:67%;" />

* 裙子长度 X，故事涨跌 Y，设定时间参数，代入互信息公式。
  * 互信息可简单理解为相关性；
  * 不相干，互信息 ≈ 0；

* 世界上很多事情彼此相关，如果它们之间**==有确定的因果关系==**，那样的信息就是**==等价的==**。
  * eg：从 A ==一定能推导出== B，那么知道 A ==等同于==知道了 B。
* 但世界上大部分相关的信息**==未必有因果==**关系，它们之间只是一种**==动态的相互关联==**的关系。
  * eg：A 发生后，B 发生的==可能性就增加==，这是相关性。

* ==**使用互信息要注意，不要把因果关系搞反了。**==
  * eg：由 A 得到 B，但 B 未必能反过来确定 A。
  * eg：乌鸦，人老特殊腐臭味；
  * eg：盖茨，扎克伯格，退学因果；

* 信息论是**对大量信息整体规律性**的描述。**相关性差的**两件事虽然可能发生，但放到**更大的时空**中，会发现纯属**偶然**。

* 如何找出事物之间更高的相关性？
  * eg：专业人士做事的方式；
    * 有很多业余人士没有的工具；
    * 风投。研究特质企业，第一次创业成功和第二次再成功之间的互信息……
    * 有意识、特定群体，投资倾斜。
  * eg：业余人士；
    * 常常只能凭感觉作判断。

* 总结
  * 介绍了**==量化度量信息相关性==**的工具：互信息。虽然不相关的事也有概率一同出现，但只有互信息高的，彼此才有较强相关性。
  * 指出了**==相关性和等价性的区别==**，以及不要在利用互信息时把两件强相关的事情之间的**==因果顺序==**颠倒了。
  * 专业人士可能因为掌握专业工具，做得比业余人士好。【何为专业工具？通用？】

* 思考题
  * 经济发展、政治制度是否存在因果关系，谁是因，谁是果？如果没有，只是相关，这种关系是否强？



2022/01/17 17:09:52 50min

------



### 15 条件熵和信息增益：你提供的信息到底值多少钱**

* 更系统、量化地分析，一条信息，到底多大价值。
  * eg：裙摆指数；
    * A 公司要并购 B 公司，**只有你一个知道**，而且你**之前对它的了解**不亚于其他人，可能你的投资会表现好一点。
    * 要注意它们**==未必构成因果关系==**，仅仅是挣到钱的可能性大一点。【很多人误区】

* 信息理论对此进行比较严格的分析：
  * X——股市的涨跌；
  * H(X)——它的全部不确定性，即 X 概率分布所对应的信息熵；
  * Y——内部消息，条件；
  * H(X|Y)——股市对于你的不确定性即 Y 条件下，C 概率分布的信息熵——条件熵；
  * H(X) 别人眼里的股市不确定性，H(X|Y) 你眼里的不确定性。
  * ==什么情况下，H(X|Y) < H(X) ?==
    * 信息论可证明，上式永远小于或等于；（Shannon 第一定律）
    * 等号成立的条件是：你得到的消息和股市无关，那些信息不会帮你也不会害你。
      * eg：预测股市的信息是你家孩子上次考试的分数。
    * 好消息（只有你知道）：
      * 如果你得到消息是股市变化的等价消息，变为确定事件—— X 等价于 Y，条件熵瞬间降为 0（即一定发生），前提是只有你知道，别人不知道。
    * 坏消息（别人也知道）：
      * 市场的有效性，**==股价能充分反映所有信息==**——一瞬间上涨——这时你再使用那条信息就挣不到钱了，即股价已经包含了市场回购的信息的反应。（同一条信息，使用一万遍，只有第一次会产生结果）——“裙摆指数”假说存在了上百年。

* 假定今天大家使用过的关于股市的各种技术指标分别是 Y1，Y2，Y3，……，YN。
  * 今天股市的不确定性不再是 H(X)，**==而是 H(X | Y1，Y2，Y3，……，YN) 的条件熵==**。【哈哈哈，如果早点知道信息论的知识，就不会那么容易在股市亏钱了，起码知道风险是有那么大的，散户是在跟市场对抗，而不是所谓的庄家】
    * 画 K 线没用，不过是众人所知的某个 Y。
    * 专业投资人没有靠画 K 线投资的【那么是如何投资的？】
    * 假如你有幸发现了一种新的信息 YM，恭喜可能挣到钱。
      * eg：彼得·林奇，PEG 信息；
      * 一旦成为公众信息，就再也挣不到钱了；
      * 因此真正能通过投资股票挣钱的人，是不会告诉你所谓投资秘诀的，电视开讲座的，反而是自己挣不到钱的。
* ==**最有效的信息已经被发现了，剩下来留给大家的微乎其微。**==

* **==信息增益（IG，Information Gain）==**
  * 信息增益 IG(Y1) = H(X) - H(X|Y1)
    * 具体到这个特殊情况，IG 就是 X 和 Y 之间的互信息，越大，说明消除不确定越大，X 和 Y 越具有相关性。
    * 另一条信息 Y2，带来的信息增益就是原来 Y1 基础上的增加，IG(Y2)。
    * 假设后面还有 N 种，带来的信息增益都是在原来所有信息基础上递减的。
  * 通常，总是率先发现所要解决问题互信息最大的信息，即增益最大的信息，越往后，增益越小。
    * eg：股市已经被人研究了几百年，有用信号已经被挖掘殆尽；
    * eg：计算机，对冲基金，数学模型，有用就保留，**==股市也很快反应了那些信息使用后的情况==**，也就是所谓的**==被 price in 了==**。

* 有幸找到一个大家都遗漏的信号，还要确定两件事：
  * 这个信号不要和其他已经采用的信号重复、或相互覆盖；
  * 这个信号带来的结果要有足够高的置信度，也就是说，如果它让你收益平均高出 1%，收益浮动区间要远小于 1%，否则浮动区间高达 10%，那一点意义也没有。

* 因此，**==在股市上系统性地捡漏近乎不可能。==**

* 信息增益的应用
  * 衡量一条==**信息的价值；**==
    * 取决于这条信息对**未知系统所带来的**信息增益，先出现的价值更大，后面更小；
    * **如果两条信息正交**，那第二条信息仍然像过去那样有价值。
  * 衡量一项**==研究发现的贡献。==**
    * eg：一项课题研究，最初几篇论文影响最大，后面论文影响力越小；
      * 两个指标：
        * 引用数量；
        * 所登载期刊的影响因子。
    * eg：媒体报道，独家报道关注程度远高于凑热闹围观；
    * eg：第一个提出，意见领袖；

* **总结**
  * 用==条件熵==的概念，解释了为什么==大众已知的信息==对投资和其他决策其实都==没有意义==；
  * 给出一个==定量==衡量每一条信息价值的尺度——==信息增益==；
  * 用上述理论解释了为什么研究领域最初的发明贡献，影响力最大。对每一个人，**==第一个发表意见，以及能够发表与众不同的意见==**，对提高自己的影响力至关重要。
  * 标新立异才有可能提供信息增量。当然，那些观点本身需要有证据支持，符合逻辑。



2022/01/17 17:45:42 35min + 19min

------



### 16 置信度：马斯克犯了什么数学错误

* eg：2016，特斯拉汽车
  * 这起车祸，出在自动驾驶功能使用了 1.3亿英里之后，而美国平均行车 0.9亿英里就出一次死亡事故。因此低于平均水平。
  * 这声明出来，一些科学家嘲笑，说马斯克数学没学好，完全没有统计中的置信度的概念。
  * 出重大车祸是随机性事件，只有当统计的数据量足够大时，从结果上判定一种车比另一种安全才有意义。【实际上，马斯克统计的是特斯拉自动驾驶从运营到出该事故的数据1.3亿，而普通交通数据是历年来的数据平均值，因此不具有可比性】
  * 麻根史丹利，估算，在美国目前的死亡事故发生的频率条件下，要想证明它的辅助驾驶更安全，需要行驶 100亿英里才能得出足够高的置信度结论。
* 置信度（Confidence Level）——衡量一个信息到底是否可靠。
  * eg：钢镚，**==你有多大把握说钢镚不均匀，正面朝上的概率更大，==**这个把握就是置信度。
  * 衡量置信度很多方法
    * T - 测试（ T 检验）——在看到==某种看似有偏差的现象==时，有==多大的可能性可以判断==这种偏差是==因为随机性造成==的，==而非真正存在偏差==。【随机性 VS 偏差】
    * 偏差确实存在——57%
    * 偶然造成——43%
    * 因此有偏差可能是真的，把自己==有多么确定这件事==量化衡量——置信度。
      * 一般认为，置信度==不到 95% 的==结论不大能相信。
    * Z - 测试

* 怎样提高置信度？
  * 通常办法就是**==增加统计的样本数量==**。

* 常犯一个错误，就是忽视它的置信度，以至于我们**==把完全随机的事情，当成必然的事情。==**
* 很多事情偶然性很大，**==几乎每一件历史上的事件，社会学上和经济学上的事情都是如此，甚至很多医学上的奇迹也是如此。==**
  * 人们总能找到合理的解释，但换一种理论也能做到这一点，因此严肃的学者们才感到证伪比证实更重要。

* 世界上有很多道理其实都很难验证，==大到==历史事件，**由于很难多次重复，总结经验其实是非常难的。**
  * ==中到==某些企业的成功经验，事后总结的自圆其说理论，换不换个环境再来一遍，都很难获得同样的成功。
  * ==小到==个人，做成一件事也有很多偶然的因素，下一次同样的方法是否可行，也有看情况而定。

* 实际上，今天真正可怕的，是那些利用超级数据中心无比强大的计算功能，以及无所不在的监控系统，和具有很强数据处理能力的大公司，它们是==人工智能背后的人==，比人工智能更可怕。

* 当普罗大众还在担心机器获得智能会不会反抗人类，==其实已经被那些智能程序控制了==。看看今天多少人自从有了微信后生活的习惯就改变了，有了今日头条后就失去了主动寻找新闻的能力，甚至失去了判断新闻真伪的能力，有了淘宝多少人买了一堆没用的便宜货。

* 总结
  * 置信度概念；人们平时看待信息常犯错误，忽视它的置信度。
  * 对于能够重复的事情，==**要被检验足够多次之后，置信度才高。**==
  * 对于难以重复检验的事情，要通过其他一些方式验证。
  * 不要总结那些根本不存在的经验，或者用更科学的话讲，就是==**别相信置信度不高的信息。**==

* 思考题
  * 平时有归纳总结，及时复盘的习惯吗？
  * 你是如何判断哪些经验值得以后借鉴，哪些经验无法复制，只能称为经历呢？



2022/01/18 17:14:05 31min

------



### 17 交叉熵：如何避免制定出于事实相反的计划

* 目的：以防我们万一作出了误判，不会像赵括一样满盘皆输。
* ==**代价函数，库尔贝勒交叉熵（K-L divergence，KL 散度）**==——信息误判时的损失。

<img src="D:\TyporaTxt\PicCopy\image-20220118171934155.png" alt="image-20220118171934155" style="zoom:67%;" />

* 信息的度量是个对数函数，差 1，实际上差了 2 倍。
  * 黑天鹅事件损失最大。

* ==**五点思考原则**==
  * 如果猜测和真实情况完全一致，没有损失。但只有==猜测不一致==，就或多或少损失。
  * 猜测和真实情况==相差越大==，损失越大。
  * ==自大的人==非常容易遗漏很多原本应该考虑的事情，赵括、马谡。
  * ==过分防范==各种情况，患得患失，除非资源丰厚、实力强大，成本很高。
  * 信息论中，任何==硬性的决定==（hard decision）都要损失信息。
    * All in；
    * 设成 0。

* 经验
  * AI ，走到最后一步之前，最好多保留一些可能性，硬性决定后失去的信息是永远不补回来的。
  * 本科以前，要通识教育；
  * 变色龙精神；
  * 对于那些可能性不大的事情，在==有所防范的同时，不要均匀分配力量。==

* 古德-图灵估计（Good-Turing Estimate），原则是从所有预见的事情中拿出很少一些资源，分配给没有预见的事情。

* 思考题
  * 说说自己随机应变把事情做成的经历，以及 “ALL IN” 做事的经历，回顾一下，看能否在下次制订计划时，得到新启发。



2022/01/18 17:33:39 19min

------



### 18 复盘：如何识别误导人的错误信息

* 掌握准确信息的好处，特别是别人信息片面的、支离破碎的，而你掌握准确全面的，优势巨大。【深有体会】

* 误导人的信息，三大特征：
  * **第一个特征：**刻意要引起你注意的人，常常会用**==耸人听闻==**的信息打动你。
    * eg：那些人也知道这样的信息提供的信息量最大，颠覆 “新知”。
  * **第二个特征：**==**没有出处。**==
    * eg：很多信息，其实是某个人断章取义，甚至肆意篡改之后，发布到所有媒体上的，只有一个不太可靠的来源，不信也罢。
  * **第三个特征：**==**缺乏上下文。**==
    * eg：“俄罗斯是世界最大产油国”，忽略了时间维度；
    * eg：页岩气革命，《页岩革命》；
  * 1、将它们放在**==更大的时空==**来考量。
    * eg：小尺度信息，就算准确，也只是**增量信息**。
  * 2、要看==**信息的一致性**==。
    * eg：标题党；
  * 3、对于从一大堆信息中抽取的信息，要看它们的**==失真率==**。
    * eg：故意误导人的==信息相反==，过滤了低频信号，保留个别高频信号，刻意渲染，失真率极高。
    * eg：印度强奸案发生率，实际排名 90多；
* 对于没有条件溯源的，怎样评价这些信息？
  * 看**==同行评议。==**

* 第一模块五个重要原则：
  * 1、最好、最重要的**==资源要用于==**那些**==出现最频繁的事情==**，这样分配资源最有效，其背后的原理是香农第一定律和哈夫曼编码。
  * 2、**==不用将相关性当成因果关系。==**弱相关性对我们做事情没什么帮助，而对于强相关性，要**==搞清楚==**谁可能是因，谁可能是果，**==切忌因果倒置。==**
  * 3、很多时候，要直接获得某种信息是很困难的，因此可以通过获得**==等价信息==**，得到同样的效果。
  * 4、日常遇到的大部分事情，都是**==渐变的==**，因此通过**==增量改进，要比推倒重来效率高==**，这就如同对增量压缩，可以比静态压缩高很多一样。
  * 5、遇到很多信息，一个比较==高效率表示信息的方法==是**==矢量化==**，即将很==多维度的信息映射到我们关心的几个维度==。eg：文字的演变就是矢量化的结果。

* 几个概念
  * **==信息熵==**，说明信息量和不确定性的关系。
  * **==冗余度==**，任何信息都有冗余，去除冗余是今天信息处理的一项重要工作，但也有==好处==，可避免出错。
  * **==不同信息的正交性：==**常说的==互补==，就是某种意义上的信息正交。同一种信息用好几遍，效果不如使用两种正交信息。不仅信息如此，打造一个团队也如此。



2022/01/18 18:02:20 30min

------

