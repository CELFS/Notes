## Task00 Paddle 深度学习总结

Date：2023/03/15 17:13:08

------



[TOC]



------



### 00 意义

​		总结的意义在于，每当习得一系列的新知识，可以从一个符合自己认知习惯和规律的层面进行加工，从而帮助大脑形成合适的认知抽象视角，即可以帮助自己找准理解新知识的角度。我更愿意把这个目标称为 “获取合适的抽象视角” 。

* 这样做的好处可以归纳为：
  * 调动学习的**主动性、参与感**，对信息进行更深的**加工**，便于认知生理结构的形成；
  * 提高**信息提取**的可能性；
  * 形成**信息快照**，提高间隔性**复盘效率**；
  * 帮助获取合适的抽象视角，便于找到对待新知的**合适认知模式**。



------



### 01 深度学习简介

* 课程：[零基础实践深度学习（第 2 版）](https://aistudio.baidu.com/aistudio/education/group/info/25302)
* 部署：[Windows 下 conda 部署 Paddle](https://www.paddlepaddle.org.cn/documentation/docs/zh/install/conda/windows-conda.html#anchor-0)
* 延伸：[进阶参考](https://www.paddlepaddle.org.cn/tutorials/projectdetail/5603475)









------



### 02 一个案例吃透深度学习

​		本章以 “手写数字识别” 为例, 从横纵结合的视角分别展开讲述了深度学习任务流程的普遍共性，包括**五个基本步骤**：数据处理、模型设计、训练配置、训练过程、保存模型。

![img](images/task00/29c753e301714502abc59e1d8318989f7a5ffdb75c11482da7df9225653ae42b.png)

​		横纵讲述的方式，就像带着地图去学习，更有方向感，也能大致知道当前步骤存在的意义。不过，学了一遍之后，可能由于学得比较仓促，而且每个小节的项目文档还没有精读，导致第二天进行回顾的时候，并**没有留下太深刻的整体印象，因此也难以掌握细枝末节**，这个过程好比刷墙，目前只是简略地从上往下跳着涂了一遍，墙上还有许多留白的洞需要补上。因此，现针对昨日笔记，进行一次复盘小结。主要内容有：

#### 01 背景知识

* 手写数字识别案例的意义
* 房价模型直接迁移带来的问题
* 横纵讲述的优点
* baseline 的意义
  * 关键：完整性

#### 02 思路分析

* 数据处理的依赖包、基本思路
* Paddle 的 API 文档
  * 关键：理清输入、输出的数据类型
  * [Paddle API 文档入口](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html)
* 如何对房价模型进行调整，以适配手写识别任务
  * 环节：模型设计、训练、测试
  * 关键：判断原有的做法是否有效

#### 03 数据处理

* 如何分析数据格式的显式特征
* 数据处理的五大操作（完整流程）
  * 读入数据、拆分样本集合
  * 训练样本集乱序、生成批次数据、校验数据有效性
* 超参数与技巧较多，例如
  * 学习用数据集（较理想） VS 开发用数据集（乱脏缺、需校验）
  * 如何选择神经网络的层数、训练过程用什么方法
  * 参数复杂度、训练步长、batch 凑齐……
  * 关键之一：未污染的测试训练集
* 模型性能的经验比较
  * 从成果的角度：不明显的提升，通常意义不大
  * 从技巧的角度：越精妙，往往越难迁移；越朴素，往往更具有通用意义
* 异步读取数据（常用）
  * 让两个环节脱钩, 并行来做, 整体会非常高效
* 数据增广
  * 局限性：任何数学技巧, 都不能弥补信息的缺失——Cornelius Lanczos
  * 关键：合理伪造

#### 04 网络结构

* 采用的神经网络不同，网络结构的将有所差异，所能解决的问题也会发生改变，这也是神经网络种类繁多的原因之一
* 类比视觉相关神经用来做听觉感知任务，效果是不如直接使用听觉相关神经来得好的，更何况输入的信号类型不一致，这也加大了迁移与学习的成本
* 关键：模型要有针对性, 匹配的才是好的

#### 05 损失函数

* 分类任务
  * 输出标签，由背后的概率分布支撑
  * 损失函数：均方误差 VS 交叉熵
    * 通常：$交叉熵 \sim \frac{1}{\text{准确率}}$ 
  * 激活函数：Softmax 函数（非线性变换）
    * Softmax 把模型的实际输出值变成概率分布值
  * 关键：最大似然思想（最小化交叉熵）

#### 06 优化算法

* 学习率
* 四种主流优化算法：
  * SGD（随机）
  * Momentum（仿 “惯性”，减震）
  * AdaGrad（动态调整，渐降）
  * Adam（思路正交，二三结合，应用广泛）
* 模型参数初始化
  * 初始值：利用预训练模型的参数（可加速网络训练、得到较高精度）

#### 07 资源配置

* 实际开发，往往需考虑 CPU、GPU 分布式训练（多卡）等资源调配问题
* 分布式训练（多卡）
  * 模型并行：一个网络模型拆分多份，分到多个设备上（GPU）训练
    * 张量并行、流水线并行
    * 数据相同，节省内存，但应用受限
  * 数据并行：一次读取多份数据，给到多个设备（GPU）上的模型
    * CPU-PRC 稀疏参数、GPU-NCCL2 稠密参数
    * 模型相同，主流用法，各设备梯度不同，需梯度同步机制
      * PRC 通信方式（Parameter server，Trainer）
      * NCCL2 通信方式（Collective）

#### 08 训练调试与优化

* 优化思路的五个关键环节
  * （1）计算分类准确率，观测模型训练效果（交叉熵 VS 准确率）
  * （2）检查模型训练过程，识别潜在问题（打印、定位）
  * （3）加入校验或测试，更好评价模型效果（三集合，欠拟合、过拟合）
  * （4）加入正则化项，避免模型过拟合（整体/局部参数 + 正则化项）
    * 防过拟合
      * 加入正则化项：增加了模型在训练集上的损失
      * 暂退法 Dropout：每次迭代，随机丢掉（屏蔽）每层若干神经元，用余下神经元继续训练
  * （5）可视化分析（打印 Loss VS matplotlib、VisualDL）

#### 09 恢复训练

* 需求：训练过程主动或被动的中断
* 保存和加载模型
  * 预测场景：只保存模型参数
  * 恢复训练场景：保存模型参数、优化器参数
* 效果：恢复训练与未中断训练的效果完全一致

#### 10 动转静部署

* 保存和加载模型之后，需对接部署工具，涉及动静态图概念
  * 声明式编程（静态图）：先编译后执行（全局）
  * 命令式编程（动态图）：解析式地执行（交互）



------

* 2023/03/15 19:12:25 2h36min 2.1-2.5
* 2023/03/15 23:44:54 3h30min 2.6-2.9
* 2023/03/16 17:27:56 2.10



------



### 03 计算机视觉基础

​		本章简要介绍了计算机视觉的内容、应用及主要挑战，结合若干子任务，分别从任务目标、模型框架、基本原理、代码实践等方面展开了分析。

​		正如人的大部分信息通过视觉获取，我们也希望让机器学会如何去 “看”。

* 因此，可以定义如下六种 “看” 的行为：
  * 图像分类（分类；CNN、Transformer）
  * 目标检测（标注位置；Anchor based、Anchor free、Transformer）
  * 图像语义识别（分割、tag；细粒度）
  * 实例分析 OCR（文字识别；两阶段、端到端算法、图网络）
  * 视频分析（分类、分割、时空、时序；帧联系）
  * 图像生成（GAN；修复、迁移、生成）
* 概念补充
  * 全连接/多层感知机
  * 卷积（互相关运算）
  * 池化（输出综合置换）
  * dropout（随机删除部分神经元）



------















